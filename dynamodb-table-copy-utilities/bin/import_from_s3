#!/usr/bin/env node
/*
 * Copyright 2014-2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 * 
 * Licensed under the Amazon Software License (the "License"). You may not use this file except in compliance with the License.
 * A copy of the License is located at
 * 
 * http://aws.amazon.com/asl/
 * 
 * or in the "LICENSE.txt" file accompanying this file.
 * 
 * This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, express or implied.
 * See the License for the specific language governing permissions and limitations under the License.
 */

var path = require('path');
var fs   = require('fs');
var lib  = path.join(path.dirname(fs.realpathSync(__filename)), '../lib');
var bin  = path.join(path.dirname(fs.realpathSync(__filename)), '../bin');
var async = require('async');
var os = require('os');
var fork = require('child_process').fork;

var util = require(lib + '/util');
var logger = util.logger;
var cliArgs = require("command-line-args");

/* define the command-line options */
var cli = cliArgs([
    { name: "debug", type: Boolean, alias: "d", description: "Set log level to debug" },
    { name: "help", type: Boolean, description: "Print usage instructions" },
    { name: "destinationEndpoint", type: String, description: "Endpoint of destination table" },
    { name: "maxKeysToReadAtOnce", type: Number, description: "Max number of S3 object keys to read in a single round" },
    { name: "args", type: Array, defaultOption: true, description: "Source S3 object prefix and destination table in the following format: s3://<bucketName>/[objectPrefix] [region:]<destination table name>" }    
]);

var options = cli.parse();
var usage = cli.getUsage({
    header: "DynamoDB Table Import utility"
});

if (options.help){
    console.log(usage);
    process.exit(0);
}

if (options.debug){
    process.env.LOG_LEVEL = 'debug';
}

var importKeysWithMultipleProcesses = function(src, dst, keys, callback){
    var nProcesses = 0;
    var nError = 0;
    var cpus = os.cpus();

    var nForks = cpus.length;
    if (keys.length < nForks){
        nForks = keys.length;
    }
    var childProcesses = {};
    var errors = 0;

    var killChildProcesses = function(code){
        for (var pid in childProcesses){
            logger.debug('Killing process %d', pid);
            childProcesses[pid].kill(code);
        }
    };

    var baseArgs = [];
    if (options.destinationEndpoint){
        baseArgs = ['--destinationEndpoint', options.destinationEndpoint];
    }
    for (var i = 0; i < nForks; i++) {
        var args = baseArgs.concat([src, dst]);

        for (var j = i; j < keys.length; j += nForks){
            args.push(keys[j]);
        }

        logger.debug('Forking process %d/%d to import %d keys', i + 1, nForks, args.length - 2);
        var proc = fork(bin + '/__import_from_s3', args);
        childProcesses[proc.pid] = proc;

        proc.on('exit', function(code){
            if (childProcesses[this.pid]){
                delete childProcesses[this.pid];
                if (code === 0){
                    logger.debug('Process %d finished successfully.', this.pid);
                } else {
                    errors++;
                    logger.error('Process %d exit with error', this.pid);
                    killChildProcesses(code);
                }
            }
            if (Object.keys(childProcesses).length === 0){
                logger.debug('No process remained.');
                if (errors > 0){
                    logger.error('Exit with code %d', errors);
                }
                process.exit(errors);
            }
        });
    }
    process.on('SIGINT', killChildProcesses);
    process.on('SIGTERM', killChildProcesses);    
};

var listS3Keys = function(bucketName, prefix, token, callback){
    var params = {
        Bucket: bucketName,
        Prefix: prefix
    };
    if (token){
        params.StartingToken = token;
    }
    util.getS3Client().listObjects(params, function(err, data){
        if (!err){
            var keys = [];
            for (var i in data.Contents){
                keys.push(data.Contents[i].Key);
            }
            callback(null, keys, data.NextToken);
        } else {
            callback(err);
        }
    });
};

var args = options.args;

if (args && args.length == 2){
    var maxKeysToReadAtOnce = 20;

    if (options.maxKeysToReadAtOnce){
        maxKeysToReadAtOnce = options.maxKeysToReadAtOnce;
    }   

    var src = util.parseS3Url(args[0]);
    var dst = util.parseDynamoDBTableName(args[1]);
    if (options.destinationEndpoint){
        dst.endpoint = options.destinationEndpoint;
    }

    logger.info('Importing data under s3://%s/%s to table %s:%s', 
        src.bucketName, src.objectPrefix, 
        dst.region, dst.tableName);

    var startTime = new Date().getTime();
    var token = null;
    async.doWhilst(
        function(done){
            listS3Keys(src.bucketName, src.objectPrefix, token, function(err, keys, nextToken){
                if(!err){
                    token = nextToken;
                    importKeysWithMultipleProcesses(args[0], args[1], keys, done);
                } else {
                    done(err);
                }
            });
        },
        function(){
            if (token){
                logger.debug('We have more keys to load. Next token is %s', token);
                return true;
            } else {
                return false;
            }
        },
        function(err){
            if (!err){
                logger.info('Imported all the items under s3://%s/%s to table %s:%s in %d ms',
                    src.bucketName, src.objectPrefix, dst.region, dst.tableName,
                    new Date().getTime() - startTime
                    );
                process.exit(0);
            } else {
                logger.error('Failed to import data: %s', err);
                process.exit(1);
            }
        }
    );
} else {
    console.error(usage);
}

