#!/usr/bin/env node
/*
 * Copyright 2014-2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 * 
 * Licensed under the Amazon Software License (the "License"). You may not use this file except in compliance with the License.
 * A copy of the License is located at
 * 
 * http://aws.amazon.com/asl/
 * 
 * or in the "LICENSE.txt" file accompanying this file.
 * 
 * This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, express or implied.
 * See the License for the specific language governing permissions and limitations under the License.
 */

var path = require('path');
var fs   = require('fs');
var lib  = path.join(path.dirname(fs.realpathSync(__filename)), '../lib');
var async = require('async');

var Constants = require(lib + '/constants');
var util = require(lib + '/util');
var logger = util.getLogger();

var DynamoDBInputStream = require(lib + '/dynamodb_input_stream');
var S3OutputStream = require(lib + '/s3_output_stream');
var OutputStreamMonitor = require(lib + '/output_stream_monitor');
var cliArgs = require("command-line-args");

/* define the command-line options */
var cli = cliArgs([
    { name: "sourceEndpoint", type: String, description: "Endpoint of source table" },
    { name: "args", type: Array, defaultOption: true, description: "Source table, destination S3 object prefix and parallel scan options in the following format: [region:]<source table name> s3://<bucketName>/[objectPrefix]  <# of total segments> <worker ID> <# of workers>" }    
]);

var options = cli.parse();

var exportToS3 = function(src, dst, segments, totalSegments, callback){
    var monitor = new OutputStreamMonitor();
    monitor.start();
    async.map(
        segments, 
        function(segment, segmentDone){
            var dynamodbInputStream = new DynamoDBInputStream(src, segment, totalSegments);
            var encoding = Constants.ENCODING_GZIP;
            var key = dst.objectPrefix + src.tableName + '_' + segment + Constants.SUFFIX_GZIP;
            var s3OutputStream = new S3OutputStream(dst.bucketName, key, encoding);

            monitor.add(s3OutputStream);

            s3OutputStream.on('finish', function(){
		        segmentDone(null, s3OutputStream.numWritten);
            });

            s3OutputStream.on('error', function(err){
                segmentDone('Failed to export segment ' + segment + ': ' + err);
            });
            dynamodbInputStream.pipe(s3OutputStream);
        }, 
        function(err, results){
            if (!err){
                monitor.stop();
                var nTotalItemsCopied = 0;
                for (var i = 0; i < results.length; i++){
                    nTotalItemsCopied += results[i];
                }
                callback(null, nTotalItemsCopied);              
            } else {
                callback(err);
            }
        }
        );
};

var args = options.args;

if (args && args.length == 5){
    var src = util.parseDynamoDBTableName(args[0]);
    var dst = util.parseS3Url(args[1]);
    var totalSegments = parseInt(args[2]);
    var workerId = parseInt(args[3]);
    var nWorkers = parseInt(args[4]);
    if (options.sourceEndpoint){
        src.endpoint = options.sourceEndpoint;  
        logger.debug('Source endpoint is set to %s', src.endpoint);
    } 

    var segments = util.getSegments(workerId, nWorkers, totalSegments);

    logger.info('Exporting table %s:%s to s3://%s/%s (segments worker %d/%d)', 
        src.region, src.tableName, 
        dst.bucketName, dst.objectPrefix,
        workerId + 1, nWorkers);

    var start = new Date().getTime();
    exportToS3(src, dst, segments, totalSegments, function(err, nItemsCopied){
        if (!err){
            logger.info('%d items exported in %d ms', nItemsCopied, new Date().getTime() - start);
            process.exit(0);
        } else {
            logger.error(err);
            process.exit(1);            
        }
    }); 
} else {
    console.error(cli.getUsage());
}

module.exports = {
    exportToS3: exportToS3
}
